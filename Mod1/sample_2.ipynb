{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pd.read_parquet('green_tripdata_2023-01.parquet')\n",
    "df_2 = pd.read_parquet('green_tripdata_2023-02.parquet')\n",
    "df = pd.concat([df_1, df_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target prepration\n",
    "\n",
    "> Our task is to predict the duration of the cab ride. So we will deduce it by two columns duration = tpep_dropoff_datetime - tpep_pickup_datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['duration'] = (df['lpep_dropoff_datetime'] - df['lpep_pickup_datetime']).dt.total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['duration'] = df['duration']/60\n",
    "\n",
    "df.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(df['duration'] >=1 ) & (df['duration'] <=60)]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer, KNNImputer\n",
    "\n",
    "\n",
    "def iterative_imputer(df:pd.DataFrame, subset_col:str, estimator = None, max_iter: int = 10, tol:float = 1e-3) -> pd.DataFrame:\n",
    "\n",
    "    \"\"\"\n",
    "        Imputes missing values in a specified column of a DataFrame using IterativeImputer.\n",
    "\n",
    "        Parameters:\n",
    "        df (pd.DataFrame): The DataFrame containing the column to impute.\n",
    "        subset_col (str): The name of the column in df to impute.\n",
    "\n",
    "        Returns:\n",
    "        pd.DataFrame: The DataFrame with imputed values in the subset_col.\n",
    "\n",
    "        Note:\n",
    "        This function assumes that subset_col is numerical. If subset_col is categorical, it should be encoded as numerical values before using this function.\n",
    "        IterativeImputer can be computationally expensive for large datasets. If runtime is a concern, consider using other imputation methods or using a subset of your data.\n",
    "    \"\"\"\n",
    "    \n",
    "    imputer = IterativeImputer(estimator=estimator, max_iter=max_iter, tol=tol)\n",
    "    imputed_values = imputer.fit_transform(df[[subset_col]])\n",
    "\n",
    "    df[subset_col] = pd.DataFrame(imputed_values, columns = [subset_col], index=df.index)\n",
    "\n",
    "    return df\n",
    "\n",
    "def KNN_imputer(df:pd.DataFrame, subset_col:str, n:int = 5) -> pd.DataFrame:\n",
    "\n",
    "    \"\"\"\n",
    "        Imputes missing values in a specified column of a DataFrame using K-Nearest Neighbors.\n",
    "\n",
    "        Parameters:\n",
    "        df (pd.DataFrame): The DataFrame containing the column to impute.\n",
    "        subset_col (str): The name of the column in df to impute.\n",
    "        n (int, optional): The number of neighbors to use for KNN imputation. Defaults to 5.\n",
    "\n",
    "        Returns:\n",
    "        pd.DataFrame: The DataFrame with imputed values in the subset_col.\n",
    "\n",
    "        Note:\n",
    "        This function assumes that subset_col is numerical. If subset_col is categorical, it should be encoded as numerical values before using this function.\n",
    "        KNN imputation can be computationally expensive for large datasets. If runtime is a concern, consider using other imputation methods or using a subset of your data.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    imputer = KNNImputer(n_neighbors = n)\n",
    "    imputed_values = imputer.fit_transform(df[[subset_col]])\n",
    "\n",
    "    df[subset_col] = pd.DataFrame(imputed_values, columns = subset_col, index=df.index)\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Several columns in our dataset contain missing values. To understand their impact on the 'duration' column, which is also our target column, we can visualize this relationship using various graphs. This will potentially reveal valuable insights, such as the importance of certain variables. It's worth noting that imputation methods, used to handle missing data, can be computationally expensive. Therefore, we need to approach this process with caution, as it will affect both the cost and time efficiency of our data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "\n",
    "sns.boxplot(x='store_and_fwd_flag', y = 'duration', data= df)\n",
    "plt.title('store_and_fwd_flag vs Duration')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Infrences\n",
    "    1. The distribution of duration for both Y and N columns are identical having median around 10 minutes, and almost identical intequartile ranges\n",
    "    2. Outliers - simmilar outliers exgtending upto 60 minutes\n",
    "\n",
    "### Conclusion \n",
    "    It is not giving any meaningful info about the data. The trips seems to have very simmilar duration patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_column = ['duration','passenger_count', 'RatecodeID', 'congestion_surcharge', 'Airport_fee']\n",
    "corr_matrix = df[numerical_column].corr()\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Infrences\n",
    "\n",
    "    1. Airport Fee is moderatley correlated with duration -- 0.47 -- Need to keep this column\n",
    "    2. Congestion_charge has negative impact on duration -- -0.18 -- Can keep, but won't see any major impact\n",
    "    3. RateCode has 0.17 impact on duration. Better to remove this\n",
    "    4. Passenger_Count to duration has no relation. So we can convininetly ignore this column\n",
    "    5. congestion charge to rate_code is -0.26, tells about relationship between pricing schemes and traffic conditions\n",
    "    6. congestion has negative relation with airport fees\n",
    "\n",
    "\n",
    "\n",
    "### Conclusion\n",
    "    Columns to keep - Airport_fee and Congestion_charge\n",
    "    Columns to drop - Passenger_count and ratecode_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['passenger_count', 'RatecodeID', 'store_and_fwd_flag' ], axis =1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(60000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_time_of_day(time):\n",
    "    hour = time.hour\n",
    "\n",
    "    if 4<=hour<10:\n",
    "        return \"Morning\"\n",
    "    \n",
    "    elif 10<=hour<16:\n",
    "        return \"Afternoon\"\n",
    "    \n",
    "    elif 16<=hour<22:\n",
    "        return \"Evening\"\n",
    "    \n",
    "    else: \n",
    "        return \"Night\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pickuptime'] = df['tpep_pickup_datetime'].apply(classify_time_of_day)\n",
    "df['droptime'] = df['tpep_dropoff_datetime'].apply(classify_time_of_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = iterative_imputer(df,subset_col='Airport_fee')\n",
    "print(\"OneColumn Done\")\n",
    "df = iterative_imputer(df,subset_col='congestion_surcharge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop([ 'tpep_pickup_datetime', 'tpep_dropoff_datetime'], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_int32(df):\n",
    "    int32_columns = df.select_dtypes(include=['int32']).columns\n",
    "\n",
    "    for col in int32_columns:\n",
    "        df[col] = df[col].astype('int64')\n",
    "    \n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "df = convert_int32(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EDA:\n",
    "\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df\n",
    "        self.numeric_df = df.select_dtypes(include=['int64','float64'])\n",
    "\n",
    "\n",
    "\n",
    "    def create_heatmap(self) -> pd.DataFrame:  \n",
    "        corr_matrix = self.numeric_df.corr()\n",
    "        plt.figure(figsize=(10,10))\n",
    "        sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', vmin=-1, vmax=1, linewidths=0.5)\n",
    "        plt.title(\"Correlation Matrix\")\n",
    "        plt.show()\n",
    "        return corr_matrix\n",
    "\n",
    "\n",
    "\n",
    "    def create_histplot(self) -> None:\n",
    "        for col in self.numeric_df:\n",
    "            plt.figure(figsize=(2,2))\n",
    "            sns.histplot(data=self.df,x=col,kde=True)\n",
    "            plt.title(f'Distribution of {col}') \n",
    "            plt.xlabel(col)\n",
    "            plt.ylabel('frequency') \n",
    "            plt.show()      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda = EDA(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix= eda.create_heatmap()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference from Heatmap\n",
    "\n",
    "    1. Few features have high relation with our target coulmn\n",
    "        a. fare_amount to duration ~ 0.8\n",
    "        b. tip_amount to duration ~ 0.52\n",
    "        c. total_amount to duration ~ 0.78\n",
    "    \n",
    "    2. Few fetaures have good relation with our target column\n",
    "        a. tolls_amount to duration ~ 0.45\n",
    "        b. Airport_fee to duration ~ 0.46\n",
    "    \n",
    "    3. Other features have relation with Target but not so high\n",
    "        a. extra  has positive relation with target column ~ [0.1,0.2]\n",
    "        b. congestion_surcharge, PULocationID, DOLocationID has negative relation with target column ~ [-0.2,-0.1]\n",
    "    \n",
    "    4. All other have very less significant relation with target\n",
    "        a. trip_distance, improvement_surcharge  < 0.1\n",
    "        b. VendorID, payment_type, mta_tax <-0.1\n",
    "    \n",
    "     \n",
    "    5. From the above heatmap we can indentify few highly correlated features\n",
    "         a. fare_amount and tip_amount ~ 0.61\n",
    "        *b. fare_amount and tolls_amount ~ 0.63\n",
    "         c. fare_amount and total_amount ~ 0.98\n",
    "        *d. fare_amount and airport_fee ~ 0.63\n",
    "         e. tip_amount and tolls_amount ~ 0.49\n",
    "         f. tip_amount and total_amount ~ 0.73\n",
    "        *g. tip_amount and airport_fee ~ 0.43\n",
    "        *h. tolls_amount and airport_fee ~ 0.48\n",
    "        *i. tolls_amount and total_amount ~ 0.71\n",
    "        *j. total_amount and airport_fee ~ 0.65\n",
    "\n",
    "\n",
    ">Note : \"*\" have been put in front of those where the impact of either or both the features is less than 0.5 which is standard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. ***Handling the Highly Correlated Features***\n",
    "    upon analysing the correlation matrix, several pairs displayed high correlations coeff. indicating possible multicolinearity, which has negative impact on model performance. To mitigate the issue e created new feautures from that capture relationship between these correlated variables and dropped the orignal features.\n",
    "\n",
    "\n",
    "> Before Applying PCA for feature reduction we need to apply Encoding, then as an additional step we use Boruta selection algorithm. Then PCA\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One Hot Encoding\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "categorical_cols = ['pickuptime','droptime']\n",
    "\n",
    "columntransformer = ColumnTransformer(transformers=[('encoder', OneHotEncoder(drop='first'), categorical_cols)], remainder='passthrough')\n",
    "\n",
    "\n",
    "encoded_df = columntransformer.fit_transform(df)\n",
    "\n",
    "encoded_df = pd.DataFrame(encoded_df, columns=columntransformer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boruta Feature Selection\n",
    "\n",
    "    Boruta is an all relevant feature selection method, while most other are minimal optimal; this means it tries to find all features carrying information usable for prediction, rather than finding a possibly compact subset of features on which some classifier has a minimal error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(encoded_df.shape)\n",
    "print(df['duration'].shape)\n",
    "\n",
    "encoded_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from boruta import BorutaPy\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor(n_jobs =-1, max_depth = 5)\n",
    "\n",
    "X = encoded_df.drop(columns=['remainder__duration']).values\n",
    "y= encoded_df['remainder__duration'].values\n",
    "\n",
    "boruta_selector = BorutaPy(rf, n_estimators='auto', random_state=42, max_iter=100, verbose=0)\n",
    "\n",
    "\n",
    "print(\"Satarting Fittin\")\n",
    "boruta_selector.fit(X,y)\n",
    "print(\"Fitting Done\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_feature = encoded_df.drop(columns=['remainder__duration']).columns[boruta_selector.ranking_ <=2]\n",
    "\n",
    "selected_df = encoded_df[selected_feature]\n",
    "\n",
    "selected_df['remainder__duration'] = encoded_df['remainder__duration']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "features = selected_df.drop(columns=['remainder__duration'])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(features)\n",
    "\n",
    "pca = PCA(n_components=5)\n",
    "pca_features = pca.fit_transform(scaled_features)\n",
    "\n",
    "pca_df = pd.DataFrame(pca_features, columns=[f'PCA{i+1}' for i in range(pca.n_components_)])\n",
    "df = pd.concat([selected_df[['remainder__duration']], pca_df], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling \n",
    "    It all comes down to the Modelling part. As the data is big and complex we were tempted to use a boosting algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, confusion_matrix, classification_report\n",
    "\n",
    "\n",
    "X = selected_df.drop(columns=['remainder__duration'])\n",
    "y = selected_df['remainder__duration']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3,random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = gbr.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_squared_error(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2 = r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "\n",
    "root_mean_squared_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
